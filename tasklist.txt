
---
когда душит жаба?
жаба душит в силу неопределённости
то есть, вот есть сумма денег, есть товары, есть приоритет.
при чётко определённых приоритетах вопросов не существует.
2150р на инструменты в Вюрте - много.
но можно ли избежать этой траты//можно ли то же самое купить дешевле? нет
вот доски на стеллаж можно было взять забесплатно - стало быть, и следовало взять
надлежит товар того же качества брать как можно дешевле, но так, чтоб прочие затраты (в т. ч. времени) не перекрывали достигаемой экономии
---
Для того, чтоб меньше нервничать и качественнее делать, надо сосредотачиваться.
Вот, в данный момент я занимаюсь ex3v2 по диссертации, занавесками по машине. Меня волнуют только ex3v2 и занавески. Ничего другого.
Остальное удостаивается максимум занесения в список, если это не что-то огонь какое срочное.
Когда закончу - может, будет волновать что-то ещё, по списку.







Лабораторный журнал

14.11.2014

Пытаемся оценить  параметры обычного диода  I=Is*(math.exp(Vd/(FT*N)) -1)


Обнаружилась проблема - Ofiura_Estimation некорректно работает со случаем, когда модель однооткликовая.
B5 получается двухразмерная [[]] и соотв. размеры матриц не подходят
Вылечено вот так
if hasattr(B5, 'A1'):
            B5=B5.A1
Решение костыльное абсолютно. Проблема лежит в преобразовании типов numpy. Что-то к чему-то сильно не тому преобразовывается


Хуже того - не хочет оценивать в случае, если N<1, да и вообще часто пробшемы с оценкой
Возможно дело в том, что параметры, находящиеся под экспонентой, плохо оцениваются..почему-то

Коэффициент неидеальности диода N лежит в пределах от 1 до 2, но чем ближе к 2, тем хуже оценивается (ситуация несколько исправляется подгонкой начального значения)
Регулярно отваливается из-за переполнения буфера

А ещё любит отваливаться по сингулярной матрице G

Пробуем  тестить априорное планирование. Регулярно отваливается по сингулярности

unoptimized-optimized: 3.10836801079e-102 6.05820816833e-113



16.11.2014
Увеличил в методе Гаусса-Ньютона количество допустимых итераций со 100 до 500. На 126 итерации при
    Ve=np.asmatrix( [0.1]   )
    xstart=[0.01]
    xend=[2]
    btrue=[1.238e-14, 1.8]
    binit=[1e-10,1.1]

Сосчитал
Для нормальной оценки
    N=30

Sk: 3.60041044444
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
 (1.2415208429083677, 2.4193403536561289, 1.5554228857954124), (0.044810052565276939, 0.1141343315902025, 0.33783772967240133))
и коэфф
[  1.23866730e-14   1.80002242e+00]

Для оценки по априорному плану
    N=20
130 итераций (хуже)
Sk: 1.48041222804 (лучше)
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
 (0.8224512378014015, 1.0960686711485694, 1.0469329831219234), (-0.11127551749409992, 0.061638370608546399, 0.24827076067984002))
и коэфф
[  1.23781918e-14   1.79999393e+00]

Что говорит о том, что в принципе априорный план лишь чуть лучше, да и то не по всем показателям.
Помним, однако, что число точек при априорной оценке на 10 меньше. Пробуем для тех же данных увеличить его до 30
Внимание! хотя в общем это и неверно, но в данном случае для нормального исследования действительно 30 (реально 31) точек в плане. Обычно их гораздо больше
Рассматривая результаты априорного планирования, мы закономерно видим, что точки начинают более равномерно распределяться по экспоненте, что позволяет лучше оценить кривизну функции.
Но случайные экспериментальные планы не сильно уступают оптимизированным.
Эксперимент с 30 точками вылетел из-за ошибки переполнения, затем выдал вот такие результаты
Sk: 4.54670794194
Iteration 143 mu=2.0 delta=[  1.88113508e-21   8.30752240e-09] deltamu=[  3.76227015e-21   1.66150448e-08] resb=[  1.23860357e-14   1.80002047e+00]
[  1.23860357e-14   1.80002047e+00]
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
(1.6238242649797059, 3.7591544757997561, 1.9388539078021727), (-0.047102022475391733, 0.14933833087683357, 0.3864431793638407))

Что, собственно говоря, даже несколько хуже, нежели полученное при нормальном исследовании



***
Важный момент - перестроить функции на работу с функцией measure, то есть той, что выдаёт результат измеренйи с погрешностью, но с btrue. Переделать надо:
 - o_p, makeMeasAccToPlan(func, expplan:list, b:list, c:dict, Ve=[], n=1, outfilename="", listOfOutvars=None) и ему подобные
 - o_sp, использующие вышеприведённые функции

Помозговать над улучшениеям критериев окончания последовательного плана
Написать штуку, которая бы по qualität - показателям выбирала наилучшую оценку, в соответствии с балльной системой (за  каждый показатель - по баллу)
и выводила бы красивые графики сравнения

В Research mode  -  графики сравнения  и проч.
В Enterprise mode  - лучшую оценку и qualität
********************************
Критичные вопросы об оценке качества
1. Как правдивее всего сравнить методы? по собственным выборкам эксп. данных или же по одной выборке, не являющейся обучающей
 ни для одного из методов, но такой, чтобы все точки были в рамках обучающих методов?

2. Если в каждой точке производить измерения некоторое число раз (интресно, какое оптимально?), затем оные данные усреднять,
как изменится реальная ковариационная матрица Ve, с которой мы работаем?

3. При проведении в каждой точке нескольких измерений появляются вот какие возможности:
    +Можно оценить, нормально ли распределение
    +Можно оценить ковариационную матрицу (собственно, мы всё время полагали, что она уже оценена и именно таким образом, но это сделано
    единственный раз и мы полагаем, что для всех такого рода изделий и измерений она одинакова)
    +И можно серьёзным образом уточнить наши измерения.
---------------------

Методы уточнения результатов оценки коэффициентов
1. Для последовательного планирования - ограничение по полке среднего логарифма функции правдоподобия
2. Для всех - прогон нескольких методов и выбор лучшего по среднему логарифма функции правдоподобия
3. Использование полученного для одного изделия последовательного плана как априорного и для изделия иного типа. Для схожих изделий с разными значениями
параметров можно данный план использовать как затравку для априорного
4. Для некоторых моделей есть точки, вызывающие переполнение. Их должно не вносить в план а итеративно перемещать, пока оне не будет нормально считаться.
5. Подгонка настроечных параметров функции gnux
6. Метод выбора начального приближения
Практика использования оценочных методов показывает, что они часто бывают весьма чувствительны к начальным показателям. Применительно к моделям полупроводниковых приборов в ходе экспериментов была обнаружена также часто случающаяся проблема возникновения ошибки переполнения при вычислениях - даже если при истинных или близких к истинным параметрам модель вычисляется нормально, при слишком далёких от истинных начальных значениях может произойти вычислительная ошибка переполнения, что приведёт к некорректным результатам, и, как следствие, неверной оценке или вообще к невозможности произвести оценку.
Для решения данной проблемы разработан метод подбора оптимального начального приближения вектора коэффициентов. Его идея
заключается в том, что, при исследовании нового типа изделий определяются границы наиболее вероятного значения вектора коэффициентов.
 Затем метод оценки прогоняется несколько раз, каждый раз со случайным начальным вектором из этого диапазона, выбираемым по равномерному распределению.
 Затем из полученных оценок выбирается наилучшая в соответствии с качественными показателями {ссылка}. После того, как по данному типу изделий набирается
 некоторая статистика, то начальное значение выбирается уже не по равномерному распределению, а по нормальному распределению со средним, равным среднему
 значению вектора коэффициентов по набранной статистике и доверительным интервалом, равным заданному диапазону.

Подход себя полностью оправдал


7. Применение инструментов с повышенной точностью вычисления
8.


Проверить многоразовые измерения в том случае, если дисперсия высока.
Вообщеспособ пахнет бредом - проще уж несколько раз в точке померить, но при этом задача технически сводится к имеющейся, лишь дисперсия меняется



Пока не очень понятно, а как extrastart, требующий range для b, совместить с sequencePlan, его не требующий, зато требующиё binit
GKNU в SP заменяется:
 +в дубовом случае на extrastart впрямую, что требует в ntries раз больше времени
 +в менее дубовом binit=середина диапазона, но тоже так себе
 +в ещё менее дубовом - сперва диапазон, а потом binit=b, собственно, сейчас вроде так.

Заключительно на 25Дек2014:
метод выбора нач. прибл. себя оправдал в классическом варианте, в SP не применён
многоразовые измерения могут дать выгоду по среднему ЛФП в 2 порядка
распределение остатков очень похоже на нормальное, теперь бы крутануть на последовательном плане, где точек побольше
и для приличия можно запостить анализатор вида распределения (проверялку на нормальность) и на нульсреднее, но в целом, скорее всего, всё ОК
Пора ваять отчёт в кошерном виде в латехе про диод с улучшениями


Написать методику формирования оптимизационной программы


Финальное разъяснение относительно теста на None:
if val is not None:
    # ...
is the Pythonic idiom for testing that a variable is not set to None.
This idiom has particular uses in the case  declaring keyword functions with default parameters.
 is tests identity in Python. Because there is one and only one instance of None present in a running Python script/program,
  is is the optimal test for this. As Johnsyweb points out, this is discussed in PEP 8 under "Programming Recommendations".

 И до кучи
 You use == when comparing values and is when comparing identities.


Я просто оставлю это здесь
всякое про отношения
http://www.livejournal.com/magazine/644533.html?xtor=EPR-1-%5BCampaign_2015_02_13%20_33_11_sending_10:12%5D-20150213-%5Bpost_10%5D-%5B11%5D
http://www.livejournal.com/magazine/447515.html

--====--===----===----====

Больше тасклиста:

возможно, имеет смысл измениьт подход к планированию - сделать ряд планов для
 - разных моделей
 - разных количеств точек

у каждого плана свои границы на коэффициенты.
Их можно стандартизировать, либо по границам, либо по истинному значению и коридору. При этом, кстати, получается искусственная ситуация - 20% коридор с истиной ТОЧНО посередине.
чтоб такого не было, хорошо истинное значение сдвигать по коридору, при пользовании библиотечным планом, или же сам план делать с коридором 20%30% или каким другим неравнозначным
2 подхода: сначала каждый файл сам кеширует свои планы
потом я беру некоторые планы,  копирую их под другим именем.
Их заношу в библиотеку. затем можно взять план уже из библиотеки

-----------------
2. Версия MPMath Limited. Nuff said
+запилен оценщик
-запилить DiodeV2Mpmath

3. Diode V2 Limited как  минимум
-полная модель с первым коэффициентом


4. Diode V2 MPMath, но есть мнение, что это будет работать нереально долго
5. Графическое представление процесса оптимизации с ограничениями - должно быть прямо-таки видно, как мы отклоняемся от границ
Может существовать в двумерном варианте и в трёхмерном варианте.
6. Графическое представление исходной кривой, точек и полученной кривой  -  чтоб было видно, как работает подгонка.
7. Вписать в диссертацию рассмотрение раздела оптимизации с ограничениями
9. Ввести временные стампы во все итерационные процессы, чтоб ну хоть как-то представлять себе, когда это закончится
10. Внедрить единый стандарт gknux, или там единую функцию для limited и unlimited для numpy (равно как и для mpmath), ибо в настоящий момент
функции организованы несколько по-разному.
11. Привести все солверы к стандарту и вообще все функции в кейсах вынести в Ofiura_CaseCreatorHelper.py по возможности
оставить только самое необходимое, которго совсем немного
этих хелперов два  - обычный и mpmath



какие именно параметры наиболее сильно и как влияют на оценку - Ve, кол-во точек в априорном, коридор границ, ещё что? после каких изменений по данным показателям выходим на плато?
 вот, кол-во точек в априорном плане. Чем их больше, тем дольше крутится метод, но в какой-то момент количество уже ничего не будет менять.





 +++++++++++++++++++
  Реального диода тред:

  =Обратный подтред:=
  Обратный не работает, потому что exp(-400), что встречается в вычислении производной, это треш и угар и не считается (выдаёт нолики)
  Реально реверсный ака обратный ток можно будет осилить наверное только при накрутке мантиссы, либо ещё чем. Ибо не строит даже ВАХ по солверу (солвер не пашет??)
  Эпичный треш: при любом значении x y в любом случае равен ноликам.
  После долгого ковыряния выяснилось вот что:
  при пробое диода его характеристика почти ступенчатая. То есть, хоть какой-то фронт можно усмотреть только при значениях напряжения, почти равных напряжению пробоя (там ещё со знаками весело, но это всё видно в текущей
  версии). Усмотреть, опять же, получается только резкий подъём вверх. Это в общем коррелирует с даташитом.
  На реальном приборе посмотреть пробой ну почти зло, хотя попробовать можно, лишь бы значения x были совсем рядом и разброс незав. переменной небольшой.
  Веселее всего тот факт, что пробой бывает тепловой, от чего диод просаживает и он умирает. Потому моделировать-то весело, а вот риальни смореть сопряжено с погибелью экспериментируемого
  Пробойные характеристики хорошо на стабилитронах смотреть, для которых пробой - штатный режим. Но и у них обратная ветка ВАХ настолько жёсткая, что диапазон x сжимается очень сильно.



  =Прямой подтред:=
  Прямой выдаёт ВАХ как на сайте manufacturer
  Можно пробовать оценить параметры на установке (хоть прямого, обратного можно подмоделировать или ещё чтол)


  =Ашотки диода подтред=
  Расширенная модель фейл, нужна переменная или хоть просто большая мантисса

 +++++++++++++++++++++
 = Транзистора тред =
 Простая модель implicit считает ну прямо очень круто - 3 минуты и вся радость (правда, требует кучу точек, но это детали)
 Куда двигаться:
  - Усложнить модель транзистора (та же модель Гуммеля - Пуна хотя б)

+++++++++++++++++++++++
=Экспериментов тред=
    =Группа экспериментов Mantissa тред=
    Назначение: обеспечить практическую поддержку разрабатываемого раздела по мантиссе
    Эксперименты:
        1. Проверка гипотезы постоянства числа итераций
        берётся стандартный кейс (диод), число проверок (100) на каждой проверке случайно выбирается binit
        и случайно выбирается btrue, пишем в список число итераций, проверяем гипотезу распределения,
        строим диаграмму рассеяния, определяем среднее, дисперсию
        2. Получение данных о точности результата на каждой итерации
        проведение 100 испытаний SimpleDiode составление таблицы
        номер итерации/число верных цифр для каждого b с сортировкой по возр, типа словарь/погрешность всего вектора/
        /длина мантиссы по погрешности с учётом порядка числа/Sk
        3.MPM DebugMeasurer MPM,
        DiodeModelAdvancedMPM
        NGEstimatorMPM с поддержкой переменной мантиссы
        проверять GMP как основу MPM
        Прогон с большой мантиссой расширенного диода, прогон с переменной мантиссой, выявление экономии времени
        и точностных характеристик

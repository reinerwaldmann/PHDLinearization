Лабораторный журнал

14.11.2014

Пытаемся оценить  параметры обычного диода  I=Is*(math.exp(Vd/(FT*N)) -1)


Обнаружилась проблема - Ofiura_Estimation некорректно работает со случаем, когда модель однооткликовая.
B5 получается двухразмерная [[]] и соотв. размеры матриц не подходят
Вылечено вот так
if hasattr(B5, 'A1'):
            B5=B5.A1
Решение костыльное абсолютно. Проблема лежит в преобразовании типов numpy. Что-то к чему-то сильно не тому преобразовывается


Хуже того - не хочет оценивать в случае, если N<1, да и вообще часто пробшемы с оценкой
Возможно дело в том, что параметры, находящиеся под экспонентой, плохо оцениваются..почему-то

Коэффициент неидеальности диода N лежит в пределах от 1 до 2, но чем ближе к 2, тем хуже оценивается (ситуация несколько исправляется подгонкой начального значения)
Регулярно отваливается из-за переполнения буфера

А ещё любит отваливаться по сингулярной матрице G

Пробуем  тестить априорное планирование. Регулярно отваливается по сингулярности

unoptimized-optimized: 3.10836801079e-102 6.05820816833e-113



16.11.2014
Увеличил в методе Гаусса-Ньютона количество допустимых итераций со 100 до 500. На 126 итерации при
    Ve=np.asmatrix( [0.1]   )
    xstart=[0.01]
    xend=[2]
    btrue=[1.238e-14, 1.8]
    binit=[1e-10,1.1]

Сосчитал
Для нормальной оценки
    N=30

Sk: 3.60041044444
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
 (1.2415208429083677, 2.4193403536561289, 1.5554228857954124), (0.044810052565276939, 0.1141343315902025, 0.33783772967240133))
и коэфф
[  1.23866730e-14   1.80002242e+00]

Для оценки по априорному плану
    N=20
130 итераций (хуже)
Sk: 1.48041222804 (лучше)
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
 (0.8224512378014015, 1.0960686711485694, 1.0469329831219234), (-0.11127551749409992, 0.061638370608546399, 0.24827076067984002))
и коэфф
[  1.23781918e-14   1.79999393e+00]

Что говорит о том, что в принципе априорный план лишь чуть лучше, да и то не по всем показателям.
Помним, однако, что число точек при априорной оценке на 10 меньше. Пробуем для тех же данных увеличить его до 30
Внимание! хотя в общем это и неверно, но в данном случае для нормального исследования действительно 30 (реально 31) точек в плане. Обычно их гораздо больше
Рассматривая результаты априорного планирования, мы закономерно видим, что точки начинают более равномерно распределяться по экспоненте, что позволяет лучше оценить кривизну функции.
Но случайные экспериментальные планы не сильно уступают оптимизированным.
Эксперимент с 30 точками вылетел из-за ошибки переполнения, затем выдал вот такие результаты
Sk: 4.54670794194
Iteration 143 mu=2.0 delta=[  1.88113508e-21   8.30752240e-09] deltamu=[  3.76227015e-21   1.66150448e-08] resb=[  1.23860357e-14   1.80002047e+00]
[  1.23860357e-14   1.80002047e+00]
('Среднее логарифма правдоподобия Дисперсия лп Сигма лп Среднее остатков Дисп. остатков Сигма остатков\n',
(1.6238242649797059, 3.7591544757997561, 1.9388539078021727), (-0.047102022475391733, 0.14933833087683357, 0.3864431793638407))

Что, собственно говоря, даже несколько хуже, нежели полученное при нормальном исследовании



#TODO хочется заняться экстракцией трёх параметров, помня, однако, о весьма специфических ошибках, возникающих в силу несовершенства методом оценки тока в неявной функции.
 Одна из суровых проблем - сильное замедление априорного планирования при использовании неявных функций.

#TODO Если говорить о суперкомпьютерах, было бы хорошо придумать интересную задачу по априорному планированию.

